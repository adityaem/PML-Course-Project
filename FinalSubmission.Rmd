---
title: Using Machine Learning to predict the manner in which they did the exercise
  based on monitor activity data
author: "AKT"
date: "December 25th, 2015"
output:
  html_document:
    fig_caption: yes
    keep_md: yes
    toc: yes
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    number_sections: yes
    toc: yes
---

##Summary
Human activity recognition research has traditionally focused on discriminating between different activities, i.e. to predict "which" activity was performed at a specific point in time (like with the Daily Living Activities dataset above). The approach we propose for the Weight Lifting Exercises dataset is to investigate "how (well)" an activity was performed by the wearer. The "how (well)" investigation has only received little attention so far, even though it potentially provides useful information for a large variety of applications,such as sports training.[Ref:http://groupware.les.inf.puc-rio.br/har#ixzz3vM4j8Pma]

##Objectives
The goal of our project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set.The model will be used to predict 20 different test cases. To develop and define any predictive model(s) we need to produce/establish the following components.

###Question -> Input data -> Features -> Algorithm -> Parameters -> Evaluation.

##Analysis Details

####1. Question - Predict the manner in which the exercise was executed based on the characteristics/dimensions of the the available data set

####2. Input Data - Now we load our base data set and perform some data exploration to understand frequency and distribution of the different variables. 

```{r}
library(lubridate)
library(ggplot2)
library(lattice)
library(AppliedPredictiveModeling)
library(caret)
library(rattle)
library(rpart.plot)
library(randomForest)

urltrain <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
TrainingDF <- read.csv(urltrain, header = TRUE, sep = ",", 
                               na.strings = c("NA", "","#DIV/0!"))
dim(TrainingDF)
colnames_train <- colnames(TrainingDF)
urltest <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
TestDF <- read.csv(urltest,header = TRUE, sep = ",", 
                               na.strings = c("NA", "","#DIV/0!"))
dim(TestDF)
colnames_test <- colnames(TestDF)
# Verify that the column names (excluding classe and problem_id) are identical in the training and test set.
all.equal(colnames_train[1:length(colnames_train)-1], colnames_test[1:length(colnames_train)-1])
```

Reviewing our Training Dataset we have determined that there are Six Particpants and Five Classes/Manners of Execution
Exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). This distribution is displayed in the Plot of Particpant Vs. Class. We see Class A being the predominant manner among all six particpants. [Ref:http://groupware.les.inf.puc-rio.br/har#ixzz3vM4j8Pma]

####Before we can identify the features we can leverage for our alogorithm and model we need to perform a cleanup and normalize for missing values & near zero values.Na values can cause unpredictable behavior and errors with ML functions. Drop un-needed columns to help with performance. Drop 1st 7 Columns as not relevant for predicting. Make sure columns are the same in both data sets.

```{r}
set.seed(12345)
# Count the number of non-NAs in each col.
nonNAs <- function(x) {
    as.vector(apply(x, 2, function(x) length(which(!is.na(x)))))
}
# Build vector of missing data or NA columns to drop.
colcnts <- nonNAs(TrainingDF)
drops <- c()
for (cnt in 1:length(colcnts)) {
    if (colcnts[cnt] < nrow(TrainingDF)) {
        drops <- c(drops, colnames_train[cnt])
    }
}
# Drop NA data and the first 7 columns as they're unnecessary for predicting.
TrainingDF <- TrainingDF[,!(names(TrainingDF) %in% drops)]
TrainingDF <- TrainingDF[,8:length(colnames(TrainingDF))]
TestDF <- TestDF[,!(names(TestDF) %in% drops)]
TestDF <- TestDF[,8:length(colnames(TestDF))]

```

####3. Features Identification and Selection-Identify the features which have the highest correlation to classe. Now we will only have relevant data for our model. These features of the data will allow us to predict the manner in which the activity was conducted based on other key data elements.

```{r}
#Feature Identification 
Results = which(names(TrainingDF) == "classe")
High_Correlation_Columns = findCorrelation(abs(cor(TrainingDF[,-Results])),0.90)
High_Correlation_Features = names(TrainingDF)[High_Correlation_Columns]
TrainingDF = TrainingDF[,-High_Correlation_Columns]
Results = which(names(TrainingDF) == "classe")
Results
```

####4. Algorithms/ 5. Parameters - Now make the dataset more manageable and meaningful by partitioning the dataset.This is  good practice as the Training Data set is much larger than the limited test cases. For our analysis we have taken a 60% of Training and 40% of Testing Data. We will also improve the quality of the data by performing data cleansing and transformations steps before we do any further analysis.

```{r}
inTrain <- createDataPartition(TrainingDF$classe, p=0.6, list=FALSE)
Training_Subset <- TrainingDF[inTrain, ]
Testing_Subset <- TrainingDF[-inTrain, ]
dim(Training_Subset); dim(Testing_Subset)
nzv <- nearZeroVar(Training_Subset, saveMetrics=TRUE)
Training_Subset <- Training_Subset[,nzv$nzv==FALSE]
nzv<- nearZeroVar(Testing_Subset,saveMetrics=TRUE)
Testing_Subset <- Testing_Subset[,nzv$nzv==FALSE]
```

####We develop two models and define the related algorithm and associated run parameters.As the outcomes are categorical, a decision tree is the first model tested using the method rpart with preprocessing and cross validation. The 2nd model developed is using the Random Forrest method also with cross validation and preprocessing. There is a risk of overifitting and preprocessing may not be needed  and may not be causing a significant improvement.

####rPart/Decision Tree  Model (Training Data set)

```{r}
modFit <- train(Training_Subset$classe ~ .,  trControl=trainControl(method = "cv", number = 4), data = Training_Subset, method="rpart")
print(modFit, digits=3)
fancyRpartPlot(modFit$finalModel,cex=.5,under.cex=1,shadow.offset=0)
predictions <- predict(modFit, newdata=Training_Subset)
print(confusionMatrix(predictions, Training_Subset$classe), digits=4)
```

####Random Forrest Model (Training Data set) Now train the model using the test dataset. Apply cross validation and preprocessing 

```{r}
modFit_rm <- train(Training_Subset$classe ~ ., method="rf", preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data=Training_Subset)
print(modFit_rm, digits=3)
predictions_rm <- predict(modFit_rm, newdata=Training_Subset)
print(confusionMatrix(predictions_rm, Training_Subset$classe), digits=4)
```

####6. Evaluation - On evaluating both the models it is observed that the Random Forrest model(method) gives a 100% accuracy with the Test Data Set Vs. the rPart(Decision Tree) method which only gives us 50% accuracy on the test data.

####rPart (Testing Data set)- Now train the model using the test dataset. Apply cross validation but no preprocessing

```{r}
modFit <- train(Testing_Subset$classe ~ .,  trControl=trainControl(method = "cv", number = 4), data = Testing_Subset, method="rpart")
print(modFit, digits=3)
predictions <- predict(modFit, newdata=Testing_Subset)
print(confusionMatrix(predictions, Testing_Subset$classe), digits=4)
```

####Random Forrest (Testing Data set)

```{r}
modFit_rm <- train(Testing_Subset$classe ~ ., method="rf", preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data=Testing_Subset)
print(modFit_rm, digits=3)
predictions_rm <- predict(modFit_rm, newdata=Testing_Subset)
print(confusionMatrix(predictions_rm, Testing_Subset$classe), digits=4)
```

####In Sample & Out of Sample Error Review

```{r}
InpredictionTesting = predict(modFit_rm, newdata = Testing_Subset)
confusionMatrix(InpredictionTesting, Testing_Subset$classe)

OutpredictionTesting = predict(modFit_rm, newdata = Training_Subset)
confusionMatrix(OutpredictionTesting, Training_Subset$classe)
```

For the random forest model used the in sample error rate is 0; the model is 100% (Reference: Random Forrest (Training Data set)) - Confusion Matrix (In Sample error based on the same data set) accurate when used on the original testing data set and 98.62 when used on another data set (Out of Sample error). This looks too good to be true and is most likely due to overfitting of the data to the model or having too small a sample size (Only 6 Patricipants) for test cases. Additionally the data set could vary based on when it was taken and is essentially a small sample of a larger homogenous dataset.

####Validate the Model Predictions against the Test Data Set.

```{r}
Validate_Prediction <- predict(modFit_rm, newdata=TestDF)
Validate_Prediction
```

##Conclusion
Random Forest was a superior model for prediction compared to rpart. The decision tree model was dependent on various variables and the interaction between them which caused the model to be inadequate. The Random Forrest model had over 98.6% accuracy and fitted well to other subsamples of the data. However, the algorithm may not have as high of accuracy on other samples,especially with a more varied subject pool. 
Overall, it is interesting to consider how monitors are affected by the way an execrise was conducted relative to other data points collected by the monitor to infer the subjects activities.
